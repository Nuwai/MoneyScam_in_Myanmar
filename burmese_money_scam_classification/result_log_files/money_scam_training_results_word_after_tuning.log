2025-03-03 16:28:07 - Training Started...
2025-03-03 16:28:07 - Training Model: Logistic Regression
2025-03-03 16:28:12 - Training Model: Decision Tree
Original Class Distribution: {0: 2481, 1: 10905, 2: 5961}
Full Training Set Size: 15477
Test Set Size: 3870
Training Pool Size: 12381
Validation Set Size: 3096

========================================
Training Model: Logistic Regression
========================================
Fold 1 Class Distribution:
label
1    5583
2    3051
0    1270
Name: count, dtype: int64
label
1    1396
2     763
0     318
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 1 (Logistic Regression)
----------------------------------------
Accuracy: 0.8716
Confusion Matrix:
 [[ 253   35   30]
 [ 117 1257   22]
 [  77   37  649]]
Classification Report:
               precision    recall  f1-score   support

           0       0.57      0.80      0.66       318
           1       0.95      0.90      0.92      1396
           2       0.93      0.85      0.89       763

    accuracy                           0.87      2477
   macro avg       0.81      0.85      0.82      2477
weighted avg       0.89      0.87      0.88      2477

F1-Score: 0.8780
ROC-AUC: 0.9612
Log Loss: 0.3633
----------------------------------------

Fold 2 Class Distribution:
label
1    5583
2    3051
0    1271
Name: count, dtype: int64
label
1    1396
2     763
0     317
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 2 (Logistic Regression)
----------------------------------------
Accuracy: 0.8647
Confusion Matrix:
 [[ 262   26   29]
 [ 130 1215   51]
 [  64   35  664]]
Classification Report:
               precision    recall  f1-score   support

           0       0.57      0.83      0.68       317
           1       0.95      0.87      0.91      1396
           2       0.89      0.87      0.88       763

    accuracy                           0.86      2476
   macro avg       0.81      0.86      0.82      2476
weighted avg       0.89      0.86      0.87      2476

F1-Score: 0.8711
ROC-AUC: 0.9645
Log Loss: 0.3625
----------------------------------------

Fold 3 Class Distribution:
label
1    5583
2    3051
0    1271
Name: count, dtype: int64
label
1    1396
2     763
0     317
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 3 (Logistic Regression)
----------------------------------------
Accuracy: 0.8704
Confusion Matrix:
 [[ 248   37   32]
 [ 119 1234   43]
 [  69   21  673]]
Classification Report:
               precision    recall  f1-score   support

           0       0.57      0.78      0.66       317
           1       0.96      0.88      0.92      1396
           2       0.90      0.88      0.89       763

    accuracy                           0.87      2476
   macro avg       0.81      0.85      0.82      2476
weighted avg       0.89      0.87      0.88      2476

F1-Score: 0.8765
ROC-AUC: 0.9641
Log Loss: 0.3525
----------------------------------------

Fold 4 Class Distribution:
label
1    5584
2    3051
0    1270
Name: count, dtype: int64
label
1    1395
2     763
0     318
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 4 (Logistic Regression)
----------------------------------------
Accuracy: 0.8720
Confusion Matrix:
 [[ 258   31   29]
 [ 126 1231   38]
 [  66   27  670]]
Classification Report:
               precision    recall  f1-score   support

           0       0.57      0.81      0.67       318
           1       0.96      0.88      0.92      1395
           2       0.91      0.88      0.89       763

    accuracy                           0.87      2476
   macro avg       0.81      0.86      0.83      2476
weighted avg       0.89      0.87      0.88      2476

F1-Score: 0.8784
ROC-AUC: 0.9664
Log Loss: 0.3495
----------------------------------------

Fold 5 Class Distribution:
label
1    5583
2    3052
0    1270
Name: count, dtype: int64
label
1    1396
2     762
0     318
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 5 (Logistic Regression)
----------------------------------------
Accuracy: 0.8724
Confusion Matrix:
 [[ 254   30   34]
 [ 126 1236   34]
 [  63   29  670]]
Classification Report:
               precision    recall  f1-score   support

           0       0.57      0.80      0.67       318
           1       0.95      0.89      0.92      1396
           2       0.91      0.88      0.89       762

    accuracy                           0.87      2476
   macro avg       0.81      0.85      0.83      2476
weighted avg       0.89      0.87      0.88      2476

F1-Score: 0.8786
ROC-AUC: 0.9629
Log Loss: 0.3634
----------------------------------------


Average F1-Score for Logistic Regression: 0.8765

========================================
Training Model: Decision Tree
========================================
Fold 1 Class Distribution:
label
1    5583
2    3051
0    1270
Name: count, dtype: int64
label
1    1396
2     763
0     318
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 1 (Decision Tree)
----------------------------------------
Accuracy: 0.8308
Confusion Matrix:
 [[ 202   69   47]
 [  86 1233   77]
 [  74   66  623]]
Classification Report:
               precision    recall  f1-score   support

           0       0.56      0.64      0.59       318
           1       0.90      0.88      0.89      1396
           2       0.83      0.82      0.83       763

    accuracy                           0.83      2477
   macro avg       0.76      0.78      0.77      2477
weighted avg       0.84      0.83      0.83      2477

F1-Score: 0.8333
ROC-AUC: 0.8482
Log Loss: 5.8793
----------------------------------------

Fold 2 Class Distribution:
label
1    5583
2    3051
0    1271
Name: count, dtype: int64
label
1    1396
2     763
0     317
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 2 (Decision Tree)
----------------------------------------
Accuracy: 0.8138
Confusion Matrix:
 [[ 176   79   62]
 [  96 1205   95]
 [  63   66  634]]
Classification Report:
               precision    recall  f1-score   support

           0       0.53      0.56      0.54       317
           1       0.89      0.86      0.88      1396
           2       0.80      0.83      0.82       763

    accuracy                           0.81      2476
   macro avg       0.74      0.75      0.74      2476
weighted avg       0.82      0.81      0.82      2476

F1-Score: 0.8154
ROC-AUC: 0.8304
Log Loss: 6.4779
----------------------------------------

Fold 3 Class Distribution:
label
1    5583
2    3051
0    1271
Name: count, dtype: int64
label
1    1396
2     763
0     317
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 3 (Decision Tree)
----------------------------------------
Accuracy: 0.8296
Confusion Matrix:
 [[ 190   91   36]
 [  99 1219   78]
 [  59   59  645]]
Classification Report:
               precision    recall  f1-score   support

           0       0.55      0.60      0.57       317
           1       0.89      0.87      0.88      1396
           2       0.85      0.85      0.85       763

    accuracy                           0.83      2476
   macro avg       0.76      0.77      0.77      2476
weighted avg       0.83      0.83      0.83      2476

F1-Score: 0.8315
ROC-AUC: 0.8447
Log Loss: 5.8853
----------------------------------------

Fold 4 Class Distribution:
label
1    5584
2    3051
0    1270
Name: count, dtype: int64
label
1    1395
2     763
0     318
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 4 (Decision Tree)
----------------------------------------
Accuracy: 0.8158
Confusion Matrix:
 [[ 185   80   53]
 [ 128 1202   65]
 [  75   55  633]]
Classification Report:
               precision    recall  f1-score   support

           0       0.48      0.58      0.52       318
           1       0.90      0.86      0.88      1395
           2       0.84      0.83      0.84       763

    accuracy                           0.82      2476
   macro avg       0.74      0.76      0.75      2476
weighted avg       0.83      0.82      0.82      2476
2025-03-03 16:28:22 - Training Model: Random Forest
2025-03-03 16:29:12 - Training Model: XGBoost

F1-Score: 0.8208
ROC-AUC: 0.8361
Log Loss: 6.1618
----------------------------------------

Fold 5 Class Distribution:
label
1    5583
2    3052
0    1270
Name: count, dtype: int64
label
1    1396
2     762
0     318
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 5 (Decision Tree)
----------------------------------------
Accuracy: 0.8235
Confusion Matrix:
 [[ 173   95   50]
 [ 107 1224   65]
 [  54   66  642]]
Classification Report:
               precision    recall  f1-score   support

           0       0.52      0.54      0.53       318
           1       0.88      0.88      0.88      1396
           2       0.85      0.84      0.85       762

    accuracy                           0.82      2476
   macro avg       0.75      0.75      0.75      2476
weighted avg       0.83      0.82      0.82      2476

F1-Score: 0.8246
ROC-AUC: 0.8358
Log Loss: 6.0313
----------------------------------------


Average F1-Score for Decision Tree: 0.8251

========================================
Training Model: Random Forest
========================================
Fold 1 Class Distribution:
label
1    5583
2    3051
0    1270
Name: count, dtype: int64
label
1    1396
2     763
0     318
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 1 (Random Forest)
----------------------------------------
Accuracy: 0.8805
Confusion Matrix:
 [[ 187   96   35]
 [  36 1341   19]
 [  36   74  653]]
Classification Report:
               precision    recall  f1-score   support

           0       0.72      0.59      0.65       318
           1       0.89      0.96      0.92      1396
           2       0.92      0.86      0.89       763

    accuracy                           0.88      2477
   macro avg       0.84      0.80      0.82      2477
weighted avg       0.88      0.88      0.88      2477

F1-Score: 0.8768
ROC-AUC: 0.9618
Log Loss: 0.3810
----------------------------------------

Fold 2 Class Distribution:
label
1    5583
2    3051
0    1271
Name: count, dtype: int64
label
1    1396
2     763
0     317
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 2 (Random Forest)
----------------------------------------
Accuracy: 0.8865
Confusion Matrix:
 [[ 196   82   39]
 [  34 1338   24]
 [  25   77  661]]
Classification Report:
               precision    recall  f1-score   support

           0       0.77      0.62      0.69       317
           1       0.89      0.96      0.92      1396
           2       0.91      0.87      0.89       763

    accuracy                           0.89      2476
   macro avg       0.86      0.81      0.83      2476
weighted avg       0.88      0.89      0.88      2476

F1-Score: 0.8832
ROC-AUC: 0.9676
Log Loss: 0.3511
----------------------------------------

Fold 3 Class Distribution:
label
1    5583
2    3051
0    1271
Name: count, dtype: int64
label
1    1396
2     763
0     317
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 3 (Random Forest)
----------------------------------------
Accuracy: 0.8881
Confusion Matrix:
 [[ 182  108   27]
 [  30 1340   26]
 [  24   62  677]]
Classification Report:
               precision    recall  f1-score   support

           0       0.77      0.57      0.66       317
           1       0.89      0.96      0.92      1396
           2       0.93      0.89      0.91       763

    accuracy                           0.89      2476
   macro avg       0.86      0.81      0.83      2476
weighted avg       0.88      0.89      0.88      2476

F1-Score: 0.8837
ROC-AUC: 0.9701
Log Loss: 0.3365
----------------------------------------

Fold 4 Class Distribution:
label
1    5584
2    3051
0    1270
Name: count, dtype: int64
label
1    1395
2     763
0     318
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 4 (Random Forest)
----------------------------------------
Accuracy: 0.8768
Confusion Matrix:
 [[ 177  108   33]
 [  45 1327   23]
 [  29   67  667]]
Classification Report:
               precision    recall  f1-score   support

           0       0.71      0.56      0.62       318
           1       0.88      0.95      0.92      1395
           2       0.92      0.87      0.90       763

    accuracy                           0.88      2476
   macro avg       0.84      0.79      0.81      2476
weighted avg       0.87      0.88      0.87      2476

F1-Score: 0.8727
ROC-AUC: 0.9667
Log Loss: 0.3505
----------------------------------------

Fold 5 Class Distribution:
label
1    5583
2    3052
0    1270
Name: count, dtype: int64
label
1    1396
2     762
0     318
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 5 (Random Forest)
----------------------------------------
Accuracy: 0.8889
Confusion Matrix:
 [[ 176   99   43]
 [  32 1344   20]
 [  25   56  681]]
Classification Report:
               precision    recall  f1-score   support

           0       0.76      0.55      0.64       318
           1       0.90      0.96      0.93      1396
           2       0.92      0.89      0.90       762

    accuracy                           0.89      2476
   macro avg       0.86      0.80      0.82      2476
weighted avg       0.88      0.89      0.88      2476

F1-Score: 0.8839
ROC-AUC: 0.9681
Log Loss: 0.3425
----------------------------------------


Average F1-Score for Random Forest: 0.8801

========================================
Training Model: XGBoost
========================================
Fold 1 Class Distribution:
label
1    5583
2    3051
0    1270
Name: count, dtype: int64
label
1    1396
2     763
0     318
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 1 (XGBoost)
----------------------------------------
Accuracy: 0.8849
Confusion Matrix:
 [[ 197   76   45]
 [  46 1329   21]
 [  34   63  666]]
Classification Report:
               precision    recall  f1-score   support

           0       0.71      0.62      0.66       318
           1       0.91      0.95      0.93      1396
           2       0.91      0.87      0.89       763

    accuracy                           0.88      2477
   macro avg       0.84      0.81      0.83      2477
weighted avg       0.88      0.88      0.88      2477

F1-Score: 0.8825
ROC-AUC: 0.9673
Log Loss: 0.3000
----------------------------------------

Fold 2 Class Distribution:
label
1    5583
2    3051
0    1271
Name: count, dtype: int64
label
1    1396
2     763
0     317
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 2 (XGBoost)
----------------------------------------
Accuracy: 0.8938
Confusion Matrix:
 [[ 212   60   45]
 [  37 1329   30]
 [  28   63  672]]
Classification Report:
               precision    recall  f1-score   support

           0       0.77      0.67      0.71       317
           1       0.92      0.95      0.93      1396
           2       0.90      0.88      0.89       763

    accuracy                           0.89      2476
   macro avg       0.86      0.83      0.85      2476
weighted avg       0.89      0.89      0.89      2476

F1-Score: 0.8919
ROC-AUC: 0.9724
Log Loss: 0.2736
----------------------------------------

Fold 3 Class Distribution:
label
1    5583
2    3051
0    1271
Name: count, dtype: int64
label
1    1396
2     763
0     317
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 3 (XGBoost)
----------------------------------------
Accuracy: 0.9006
Confusion Matrix:
 [[ 198   86   33]
 [  29 1342   25]
 [  24   49  690]]
Classification Report:
               precision    recall  f1-score   support

           0       0.79      0.62      0.70       317
           1       0.91      0.96      0.93      1396
           2       0.92      0.90      0.91       763

    accuracy                           0.90      2476
   macro avg       0.87      0.83      0.85      2476
weighted avg       0.90      0.90      0.90      2476
2025-03-03 16:30:43 - Training Model: SVM
2025-03-03 16:35:00 - Training Model: Naive Bayes

F1-Score: 0.8974
ROC-AUC: 0.9723
Log Loss: 0.2712
----------------------------------------

Fold 4 Class Distribution:
label
1    5584
2    3051
0    1270
Name: count, dtype: int64
label
1    1395
2     763
0     318
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 4 (XGBoost)
----------------------------------------
Accuracy: 0.8934
Confusion Matrix:
 [[ 202   77   39]
 [  42 1325   28]
 [  23   55  685]]
Classification Report:
               precision    recall  f1-score   support

           0       0.76      0.64      0.69       318
           1       0.91      0.95      0.93      1395
           2       0.91      0.90      0.90       763

    accuracy                           0.89      2476
   macro avg       0.86      0.83      0.84      2476
weighted avg       0.89      0.89      0.89      2476

F1-Score: 0.8909
ROC-AUC: 0.9696
Log Loss: 0.2846
----------------------------------------

Fold 5 Class Distribution:
label
1    5583
2    3052
0    1270
Name: count, dtype: int64
label
1    1396
2     762
0     318
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 5 (XGBoost)
----------------------------------------
Accuracy: 0.8914
Confusion Matrix:
 [[ 193   75   50]
 [  44 1328   24]
 [  25   51  686]]
Classification Report:
               precision    recall  f1-score   support

           0       0.74      0.61      0.67       318
           1       0.91      0.95      0.93      1396
           2       0.90      0.90      0.90       762

    accuracy                           0.89      2476
   macro avg       0.85      0.82      0.83      2476
weighted avg       0.89      0.89      0.89      2476

F1-Score: 0.8883
ROC-AUC: 0.9718
Log Loss: 0.2730
----------------------------------------


Average F1-Score for XGBoost: 0.8902

========================================
Training Model: SVM
========================================
Fold 1 Class Distribution:
label
1    5583
2    3051
0    1270
Name: count, dtype: int64
label
1    1396
2     763
0     318
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 1 (SVM)
----------------------------------------
Accuracy: 0.8857
Confusion Matrix:
 [[ 243   44   31]
 [  89 1287   20]
 [  57   42  664]]
Classification Report:
               precision    recall  f1-score   support

           0       0.62      0.76      0.69       318
           1       0.94      0.92      0.93      1396
           2       0.93      0.87      0.90       763

    accuracy                           0.89      2477
   macro avg       0.83      0.85      0.84      2477
weighted avg       0.89      0.89      0.89      2477

F1-Score: 0.8889
ROC-AUC: 0.9662
Log Loss: 0.3047
----------------------------------------

Fold 2 Class Distribution:
label
1    5583
2    3051
0    1271
Name: count, dtype: int64
label
1    1396
2     763
0     317
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 2 (SVM)
----------------------------------------
Accuracy: 0.8877
Confusion Matrix:
 [[ 251   34   32]
 [  92 1264   40]
 [  36   44  683]]
Classification Report:
               precision    recall  f1-score   support

           0       0.66      0.79      0.72       317
           1       0.94      0.91      0.92      1396
           2       0.90      0.90      0.90       763

    accuracy                           0.89      2476
   macro avg       0.84      0.86      0.85      2476
weighted avg       0.89      0.89      0.89      2476

F1-Score: 0.8902
ROC-AUC: 0.9677
Log Loss: 0.2964
----------------------------------------

Fold 3 Class Distribution:
label
1    5583
2    3051
0    1271
Name: count, dtype: int64
label
1    1396
2     763
0     317
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 3 (SVM)
----------------------------------------
Accuracy: 0.8974
Confusion Matrix:
 [[ 243   46   28]
 [  69 1302   25]
 [  46   40  677]]
Classification Report:
               precision    recall  f1-score   support

           0       0.68      0.77      0.72       317
           1       0.94      0.93      0.94      1396
           2       0.93      0.89      0.91       763

    accuracy                           0.90      2476
   macro avg       0.85      0.86      0.85      2476
weighted avg       0.90      0.90      0.90      2476

F1-Score: 0.8990
ROC-AUC: 0.9700
Log Loss: 0.2846
----------------------------------------

Fold 4 Class Distribution:
label
1    5584
2    3051
0    1270
Name: count, dtype: int64
label
1    1395
2     763
0     318
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 4 (SVM)
----------------------------------------
Accuracy: 0.8897
Confusion Matrix:
 [[ 252   37   29]
 [  97 1273   25]
 [  60   25  678]]
Classification Report:
               precision    recall  f1-score   support

           0       0.62      0.79      0.69       318
           1       0.95      0.91      0.93      1395
           2       0.93      0.89      0.91       763

    accuracy                           0.89      2476
   macro avg       0.83      0.86      0.84      2476
weighted avg       0.90      0.89      0.89      2476

F1-Score: 0.8940
ROC-AUC: 0.9690
Log Loss: 0.2883
----------------------------------------

Fold 5 Class Distribution:
label
1    5583
2    3052
0    1270
Name: count, dtype: int64
label
1    1396
2     762
0     318
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 5 (SVM)
----------------------------------------
Accuracy: 0.8901
Confusion Matrix:
 [[ 238   41   39]
 [  98 1273   25]
 [  37   32  693]]
Classification Report:
               precision    recall  f1-score   support

           0       0.64      0.75      0.69       318
           1       0.95      0.91      0.93      1396
           2       0.92      0.91      0.91       762

    accuracy                           0.89      2476
   macro avg       0.83      0.86      0.84      2476
weighted avg       0.90      0.89      0.89      2476

F1-Score: 0.8928
ROC-AUC: 0.9671
Log Loss: 0.2930
----------------------------------------


Average F1-Score for SVM: 0.8930

========================================
Training Model: Naive Bayes
========================================
Fold 1 Class Distribution:
label
1    5583
2    3051
0    1270
Name: count, dtype: int64
label
1    1396
2     763
0     318
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 1 (Naive Bayes)
----------------------------------------
Accuracy: 0.8571
Confusion Matrix:
 [[ 178   90   50]
 [  63 1302   31]
 [  20  100  643]]
Classification Report:
               precision    recall  f1-score   support

           0       0.68      0.56      0.61       318
           1       0.87      0.93      0.90      1396
           2       0.89      0.84      0.86       763

    accuracy                           0.86      2477
   macro avg       0.81      0.78      0.79      2477
weighted avg       0.85      0.86      0.85      2477

F1-Score: 0.8535
ROC-AUC: 0.9552
Log Loss: 0.3638
----------------------------------------

Fold 2 Class Distribution:
label
1    5583
2    3051
0    1271
Name: count, dtype: int64
label
1    1396
2     763
0     317
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 2 (Naive Bayes)
----------------------------------------
Accuracy: 0.8655
Confusion Matrix:
 [[ 177   92   48]
 [  54 1310   32]
 [   9   98  656]]
Classification Report:
               precision    recall  f1-score   support

           0       0.74      0.56      0.64       317
           1       0.87      0.94      0.90      1396
           2       0.89      0.86      0.88       763

    accuracy                           0.87      2476
   macro avg       0.83      0.79      0.81      2476
weighted avg       0.86      0.87      0.86      2476

F1-Score: 0.8612
ROC-AUC: 0.9631
Log Loss: 0.3371
----------------------------------------

Fold 3 Class Distribution:
label
1    5583
2    3051
0    1271
Name: count, dtype: int64
label
1    1396
2     763
0     317
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 3 (Naive Bayes)
----------------------------------------
Accuracy: 0.8683
Confusion Matrix:
 [[ 165   95   57]
 [  49 1312   35]
 [   8   82  673]]
Classification Report:
               precision    recall  f1-score   support

           0       0.74      0.52      0.61       317
           1       0.88      0.94      0.91      1396
           2       0.88      0.88      0.88       763

    accuracy                           0.87      2476
   macro avg       0.83      0.78      0.80      2476
weighted avg       0.86      0.87      0.86      2476

F1-Score: 0.8626
ROC-AUC: 0.9601
Log Loss: 0.3437
----------------------------------------

Fold 4 Class Distribution:
label
1    5584
2    3051
0    1270
Name: count, dtype: int64
label
1    1395
2     763
0     318
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 4 (Naive Bayes)
----------------------------------------
Accuracy: 0.8647
Confusion Matrix:
 [[ 169  104   45]
 [  54 1306   35]
 [  11   86  666]]
Classification Report:
               precision    recall  f1-score   support

           0       0.72      0.53      0.61       318
           1       0.87      0.94      0.90      1395
           2       0.89      0.87      0.88       763

    accuracy                           0.86      2476
   macro avg       0.83      0.78      0.80      2476
weighted avg       0.86      0.86      0.86      2476

F1-Score: 0.8597
ROC-AUC: 0.9607
Log Loss: 0.3447
----------------------------------------

Fold 5 Class Distribution:
label
1    5583
2    3052
0    1270
Name: count, dtype: int64
label
1    1396
2     762
0     318
Name: count, dtype: int64

----------------------------------------
Evaluating Fold 5 (Naive Bayes)
----------------------------------------
Accuracy: 0.8647
Confusion Matrix:
 [[ 174  100   44]
 [  55 1301   40]
 [   7   89  666]]
Classification Report:
               precision    recall  f1-score   support

           0       0.74      0.55      0.63       318
           1       0.87      0.93      0.90      1396
           2       0.89      0.87      0.88       762

    accuracy                           0.86      2476
   macro avg       0.83      0.78      0.80      2476
weighted avg       0.86      0.86      0.86      2476

F1-Score: 0.8601
ROC-AUC: 0.9592
Log Loss: 0.3506
----------------------------------------


Average F1-Score for Naive Bayes: 0.8594

======================================================================
Model Performance Summary
======================================================================
+---------------------+--------------------+
| Model               |   Average F1-Score |
+=====================+====================+
| Logistic Regression |             0.8765 |
+---------------------+--------------------+
| Decision Tree       |             0.8251 |
+---------------------+--------------------+
| Random Forest       |             0.8801 |
+---------------------+--------------------+
| XGBoost             |             0.8902 |
+---------------------+--------------------+
| SVM                 |             0.893  |
+---------------------+--------------------+
| Naive Bayes         |             0.8594 |
+---------------------+--------------------+

======================================================================
Best Model: SVM (F1-Score: 0.8930)
======================================================================

========================================
Hyperparameter Tuning for SVM
========================================
Best Hyperparameters for SVM: {'classifier__kernel': 'rbf', 'classifier__gamma': 1, 'classifier__degree': 4, 'classifier__C': 1}

========================================
Final Evaluation on Test Set
========================================
Accuracy: 0.8990
Confusion Matrix:
 [[ 399   51   46]
 [ 112 2022   47]
 [  74   61 1058]]
Classification Report:
               precision    recall  f1-score   support

           0       0.68      0.80      0.74       496
           1       0.95      0.93      0.94      2181
           2       0.92      0.89      0.90      1193

    accuracy                           0.90      3870
   macro avg       0.85      0.87      0.86      3870
weighted avg       0.90      0.90      0.90      3870

F1-Score: 0.9011
ROC-AUC: 0.9728
Log Loss: 0.2733
----------------------------------------


Training Complete. Logs saved in: C:\Users\nuwai\Documents\Sophia_Skill_Development\Sophia_projects\Burmese Scam Detector\burmese_money_scam_classification\money_scam_training_results.log
